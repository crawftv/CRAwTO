{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-04-11T23:02:42.759Z",
     "iopub.status.busy": "2020-04-11T23:02:42.689Z",
     "iopub.status.idle": "2020-04-11T23:02:52.706Z",
     "shell.execute_reply": "2020-04-11T23:02:52.781Z"
    }
   },
   "outputs": [],
   "source": [
    "from prefect import Flow, Parameter, unmapped\n",
    "import pandas as pd\n",
    "from prefect.engine.executors import DaskExecutor\n",
    "from ml_flow import data_cleaning_flow\n",
    "from tinydb import TinyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "test= pd.read_csv(\"../data/titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-13 02:39:40,272] INFO - prefect.FlowRunner | Beginning Flow run for 'data_cleaning'\n",
      "[2020-04-13 02:39:40,277] INFO - prefect.FlowRunner | Starting flow run.\n",
      "[2020-04-13 02:39:40,540] INFO - prefect.TaskRunner | Task 'input_data': Starting task run...\n",
      "[2020-04-13 02:39:40,548] INFO - prefect.TaskRunner | Task 'input_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,668] INFO - prefect.TaskRunner | Task 'target': Starting task run...\n",
      "[2020-04-13 02:39:40,703] INFO - prefect.TaskRunner | Task 'target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,733] INFO - prefect.TaskRunner | Task 'problem': Starting task run...\n",
      "[2020-04-13 02:39:40,736] INFO - prefect.TaskRunner | Task 'features': Starting task run...\n",
      "[2020-04-13 02:39:40,772] INFO - prefect.TaskRunner | Task 'problem': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,774] INFO - prefect.TaskRunner | Task 'extract_nan_features': Starting task run...\n",
      "[2020-04-13 02:39:40,786] INFO - prefect.TaskRunner | Task 'features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,796] INFO - prefect.TaskRunner | Task 'extract_problematic_features': Starting task run...\n",
      "[2020-04-13 02:39:40,851] INFO - prefect.TaskRunner | Task 'extract_nan_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,861] INFO - prefect.TaskRunner | Task 'extract_problematic_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,882] INFO - prefect.TaskRunner | Task 'extract_undefined_features': Starting task run...\n",
      "[2020-04-13 02:39:40,889] INFO - prefect.TaskRunner | Task 'extract_undefined_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,923] INFO - prefect.TaskRunner | Task 'fit_transform_missing_indicator': Starting task run...\n",
      "['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked'][2020-04-13 02:39:40,927] INFO - prefect.TaskRunner | Task 'extract_categorical_features': Starting task run...\n",
      "\n",
      "[3 8]\n",
      "['missing_Age', 'missing_Embarked']\n",
      "[2020-04-13 02:39:40,935] INFO - prefect.TaskRunner | Task 'extract_numeric_features': Starting task run...\n",
      "[2020-04-13 02:39:40,955] INFO - prefect.TaskRunner | Task 'fit_transform_missing_indicator': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,966] INFO - prefect.TaskRunner | Task 'extract_numeric_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,974] INFO - prefect.TaskRunner | Task 'extract_categorical_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:40,998] INFO - prefect.TaskRunner | Task 'extract_train_valid_split': Starting task run...\n",
      "[2020-04-13 02:39:41,006] INFO - prefect.TaskRunner | Task 'extract_train_valid_split': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,034] INFO - prefect.TaskRunner | Task 'extract_valid_data': Starting task run...\n",
      "[2020-04-13 02:39:41,037] INFO - prefect.TaskRunner | Task 'extract_train_data': Starting task run...\n",
      "[2020-04-13 02:39:41,044] INFO - prefect.TaskRunner | Task 'extract_valid_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,052] INFO - prefect.TaskRunner | Task 'extract_train_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,079] INFO - prefect.TaskRunner | Task 'fit_categorical_imputer': Starting task run...\n",
      "[2020-04-13 02:39:41,099] INFO - prefect.TaskRunner | Task 'fit_target_transformer': Starting task run...\n",
      "[2020-04-13 02:39:41,100] INFO - prefect.TaskRunner | Task 'fit_numeric_imputer': Starting task run...\n",
      "[2020-04-13 02:39:41,110] INFO - prefect.TaskRunner | Task 'fit_target_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,121] INFO - prefect.TaskRunner | Task 'fit_numeric_imputer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,128] INFO - prefect.TaskRunner | Task 'fit_categorical_imputer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,190] INFO - prefect.TaskRunner | Task 'impute_numeric_df': Starting task run...\n",
      "[2020-04-13 02:39:41,196] INFO - prefect.TaskRunner | Task 'transform_categorical_data': Starting task run...\n",
      "[2020-04-13 02:39:41,204] INFO - prefect.TaskRunner | Task 'transform_categorical_data': Starting task run...\n",
      "[2020-04-13 02:39:41,205] INFO - prefect.TaskRunner | Task 'transform_target': Starting task run...\n",
      "[2020-04-13 02:39:41,207] INFO - prefect.TaskRunner | Task 'impute_numeric_df': Starting task run...\n",
      "[2020-04-13 02:39:41,226] INFO - prefect.TaskRunner | Task 'impute_numeric_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,229] INFO - prefect.TaskRunner | Task 'transform_target': Starting task run...\n",
      "[2020-04-13 02:39:41,240] INFO - prefect.TaskRunner | Task 'transform_categorical_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,250] INFO - prefect.TaskRunner | Task 'transform_categorical_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,257] INFO - prefect.TaskRunner | Task 'transform_target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,269] INFO - prefect.TaskRunner | Task 'impute_numeric_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,278] INFO - prefect.TaskRunner | Task 'transform_target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,319] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 02:39:41,325] INFO - prefect.TaskRunner | Task 'fit_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 02:39:41,340] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 02:39:41,346] INFO - prefect.TaskRunner | Task 'fit_target_encoder': Starting task run...\n",
      "[2020-04-13 02:39:41,348] ERROR - prefect.TaskRunner | Unexpected error: AttributeError(\"'Series' object has no attribute 'to_feather'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\runner.py\", line 48, in inner\n",
      "    new_state = method(self, state, *args, **kwargs)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\task_runner.py\", line 883, in get_task_run_state\n",
      "    result = timeout_handler(\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\utilities\\executors.py\", line 185, in timeout_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"E:\\projects\\crawto\\crawto\\ml_flow.py\", line 281, in save_data\n",
      "    df.to_feather(path)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'Series' object has no attribute 'to_feather'\n",
      "[2020-04-13 02:39:41,361] ERROR - prefect.TaskRunner | Unexpected error: AttributeError(\"'Series' object has no attribute 'to_feather'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\runner.py\", line 48, in inner\n",
      "    new_state = method(self, state, *args, **kwargs)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\task_runner.py\", line 883, in get_task_run_state\n",
      "    result = timeout_handler(\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\utilities\\executors.py\", line 185, in timeout_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"E:\\projects\\crawto\\crawto\\ml_flow.py\", line 281, in save_data\n",
      "    df.to_feather(path)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'Series' object has no attribute 'to_feather'\n",
      "[2020-04-13 02:39:41,369] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Failed'\n",
      "[2020-04-13 02:39:41,385] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Failed'\n",
      "[2020-04-13 02:39:41,406] INFO - prefect.TaskRunner | Task 'fit_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-13 02:39:41,469] INFO - prefect.TaskRunner | Task 'fit_target_encoder': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,518] INFO - prefect.TaskRunner | Task 'target_encoder_transform': Starting task run...\n",
      "[2020-04-13 02:39:41,519] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 02:39:41,531] INFO - prefect.TaskRunner | Task 'target_encoder_transform': Starting task run...\n",
      "[2020-04-13 02:39:41,532] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 02:39:41,545] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,566] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,577] INFO - prefect.TaskRunner | Task 'target_encoder_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,588] INFO - prefect.TaskRunner | Task 'target_encoder_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,606] INFO - prefect.TaskRunner | Task 'merge_transformed_data': Starting task run...\n",
      "[2020-04-13 02:39:41,629] INFO - prefect.TaskRunner | Task 'merge_transformed_data': Starting task run...\n",
      "[2020-04-13 02:39:41,630] INFO - prefect.TaskRunner | Task 'merge_transformed_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,648] INFO - prefect.TaskRunner | Task 'merge_transformed_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:41,654] INFO - prefect.TaskRunner | Task 'fit_hbos_transformer': Starting task run...\n",
      "[2020-04-13 02:39:43,010] INFO - prefect.TaskRunner | Task 'fit_hbos_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,029] INFO - prefect.TaskRunner | Task 'hbos_transform': Starting task run...\n",
      "[2020-04-13 02:39:43,041] INFO - prefect.TaskRunner | Task 'hbos_transform': Starting task run...\n",
      "[2020-04-13 02:39:43,050] INFO - prefect.TaskRunner | Task 'hbos_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,058] INFO - prefect.TaskRunner | Task 'hbos_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,074] INFO - prefect.TaskRunner | Task 'merge_hbos_df': Starting task run...\n",
      "[2020-04-13 02:39:43,090] INFO - prefect.TaskRunner | Task 'merge_hbos_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,097] INFO - prefect.TaskRunner | Task 'merge_hbos_df': Starting task run...\n",
      "[2020-04-13 02:39:43,119] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 02:39:43,121] INFO - prefect.TaskRunner | Task 'merge_hbos_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,138] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 02:39:43,148] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:43,156] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 02:39:45,177] INFO - prefect.FlowRunner | Flow run FAILED: some reference tasks failed.\n"
     ]
    }
   ],
   "source": [
    "executor = DaskExecutor()\n",
    "data_cleaner = data_cleaning_flow.run(\n",
    "    input_data= input_df, \n",
    "    problem=\"binary classification\", \n",
    "    target = \"Survived\", \n",
    "    features = \"infer\",\n",
    "    executor=executor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_svd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e2ced53f29ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtransformed_train_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transformed_train.df\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtransformed_valid_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transformed_valid.df\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_svd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_train_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msvd_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transformed_train_df\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtinydb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msvd_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_valid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transformed_valid_df\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtinydb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fit_svd' is not defined"
     ]
    }
   ],
   "source": [
    "with Flow(\"data_visualization\") as flow:\n",
    "    transformed_train_df = pd.read_feather(\"transformed_train.df\")\n",
    "    transformed_valid_df = pd.read_feather(\"transformed_valid.df\")\n",
    "    svd = fit_svd(transformed_train_df)\n",
    "    svd_train = svd_transform(svd, transformed_train_df, \"transformed_train_df\",tinydb)\n",
    "    svd_valid = svd_transform(svd, transformed_valid_df, \"transformed_valid_df\",tinydb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_model import MetaModel, meta_model_flow\n",
    "from tinydb import TinyDB\n",
    "tinydb = TinyDB(\"db.json\")\n",
    "meta_model = MetaModel(problem=\"regression\", db=tinydb,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "# d =cloudpickle.dumps(meta_model)\n",
    "# cloudpickle.loads(d)\n",
    "d = cloudpickle.dumps(meta_model.models[0])\n",
    "cloudpickle.loads(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x05\\x95\\x9c\\r\\x00\\x00\\x00\\x00\\x00\\x00]\\x94(\\x8c\\nmeta_model\\x94\\x8c\\x05Model\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x07problem\\x94\\x8c\\nregression\\x94\\x8c\\x05model\\x94\\x8c(sklearn.linear_model._coordinate_descent\\x94\\x8c\\nElasticNet\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x05alpha\\x94G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08l1_ratio\\x94G?\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\rfit_intercept\\x94\\x88\\x8c\\tnormalize\\x94\\x89\\x8c\\nprecompute\\x94\\x89\\x8c\\x08max_iter\\x94M\\xe8\\x03\\x8c\\x06copy_X\\x94\\x88\\x8c\\x03tol\\x94G?\\x1a6\\xe2\\xeb\\x1cC-\\x8c\\nwarm_start\\x94\\x89\\x8c\\x08positive\\x94\\x89\\x8c\\x0crandom_state\\x94N\\x8c\\tselection\\x94\\x8c\\x06cyclic\\x94\\x8c\\x10_sklearn_version\\x94\\x8c\\x0c0.22.2.post1\\x94ub\\x8c\\nparam_hash\\x94\\x8c)<sha256 HASH object @ 0x000001640BBA9C70>\\x94\\x8c\\x03uid\\x94\\x8c\\x04uuid\\x94\\x8c\\x04UUID\\x94\\x93\\x94)\\x81\\x94}\\x94\\x8c\\x03int\\x94\\x8a\\x10a\\xce\\xf7\\x87#\\x1e)\\xa9GEO\\xcb.\\x0b\\xd4/sb\\x8c\\x02db\\x94\\x8c\\x0ftinydb.database\\x94\\x8c\\x06TinyDB\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\n_cls_table\\x94h\\'\\x8c\\x05Table\\x94\\x93\\x94\\x8c\\x12_cls_storage_proxy\\x94h\\'\\x8c\\x0cStorageProxy\\x94\\x93\\x94\\x8c\\x08_storage\\x94\\x8c\\x0ftinydb.storages\\x94\\x8c\\x0bJSONStorage\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x06kwargs\\x94}\\x94\\x8c\\x07_handle\\x94\\x8c\\x1ccloudpickle.cloudpickle_fast\\x94\\x8c\\x13_file_reconstructor\\x94\\x93\\x94\\x8c\\x03_io\\x94\\x8c\\x08StringIO\\x94\\x93\\x94)\\x81\\x94(\\x8c\\x10{\"_default\": {}}\\x94\\x8c\\x01\\n\\x94K\\x10}\\x94\\x8c\\x04name\\x94\\x8c\\x07db.json\\x94st\\x94b\\x85\\x94R\\x94ub\\x8c\\x07_opened\\x94\\x88\\x8c\\x0c_table_cache\\x94}\\x94\\x8c\\x08_default\\x94h.)\\x81\\x94}\\x94(h2h1)\\x81\\x94}\\x94(h2h6\\x8c\\x0b_table_name\\x94hMub\\x8c\\x05_name\\x94hM\\x8c\\x0c_query_cache\\x94\\x8c\\x0ctinydb.utils\\x94\\x8c\\x08LRUCache\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08capacity\\x94K\\n\\x8c\\x10_LRUCache__cache\\x94\\x8c\\x0bcollections\\x94\\x8c\\x0bOrderedDict\\x94\\x93\\x94)R\\x94ub\\x8c\\x08_last_id\\x94K\\x00ubs\\x8c\\x06_table\\x94hNubhE\\x8cb<class \\'sklearn.linear_model._coordinate_descent.ElasticNet\\'>-2fd40b2e-cb4f-4547-a929-1e2387f7ce61\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x1asklearn.linear_model._base\\x94\\x8c\\x10LinearRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x10\\x88h\\x11\\x89h\\x14\\x88\\x8c\\x06n_jobs\\x94Nh\\x1bh\\x1cubh\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA9C30>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x11\\xd2-cz\\xac\\xb6\\xbe\\x9c=L\\x00\\xf4{\\xdf\\t\\xcb\\x00sbh&h*hE\\x8cZ<class \\'sklearn.linear_model._base.LinearRegression\\'>-cb09df7b-f400-4c3d-9cbe-b6ac7a632dd2\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x0ebaseline_model\\x94\\x8c\\x1cBaselineRegressionPrediction\\x94\\x93\\x94)\\x81\\x94h\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA9C30>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x11*\\x96\\xf8\\xecRJv\\x8dBC\\xd9\\x81\\xa6M\\xb5\\xfe\\x00sbh&h*hE\\x8cZ<class \\'baseline_model.BaselineRegressionPrediction\\'>-feb54da6-81d9-4342-8d76-4a52ecf8962a\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x15sklearn.tree._classes\\x94\\x8c\\x15DecisionTreeRegressor\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\tcriterion\\x94\\x8c\\x03mse\\x94\\x8c\\x08splitter\\x94\\x8c\\x04best\\x94\\x8c\\tmax_depth\\x94N\\x8c\\x11min_samples_split\\x94K\\x02\\x8c\\x10min_samples_leaf\\x94K\\x01\\x8c\\x18min_weight_fraction_leaf\\x94G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x0cmax_features\\x94Nh\\x18N\\x8c\\x0emax_leaf_nodes\\x94N\\x8c\\x15min_impurity_decrease\\x94G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x12min_impurity_split\\x94N\\x8c\\x0cclass_weight\\x94N\\x8c\\x07presort\\x94\\x8c\\ndeprecated\\x94\\x8c\\tccp_alpha\\x94G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x1bh\\x1cubh\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA98F0>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x10\\xbd\\x1c`\\xc3V\\xf3\\xd1\\x9e:E\\xa2\\x82t\\x1fU\\x03sbh&h*hE\\x8cZ<class \\'sklearn.tree._classes.DecisionTreeRegressor\\'>-03551f74-82a2-453a-9ed1-f356c3601cbd\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x1bsklearn.linear_model._ridge\\x94\\x8c\\x05Ridge\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x0eG?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00h\\x10\\x88h\\x11\\x89h\\x14\\x88h\\x13Nh\\x15G?PbM\\xd2\\xf1\\xa9\\xfc\\x8c\\x06solver\\x94\\x8c\\x04auto\\x94h\\x18Nh\\x1bh\\x1cubh\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA9C50>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x11^\\x9b\\x04:l\\xf7\\x16\\x92\\xd4Op\\xf4\\xbev\\xa3\\xd7\\x00sbh&h*hE\\x8cP<class \\'sklearn.linear_model._ridge.Ridge\\'>-d7a376be-f470-4fd4-9216-f76c3a049b5e\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x14sklearn.ensemble._gb\\x94\\x8c\\x19GradientBoostingRegressor\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x0cn_estimators\\x94Kd\\x8c\\rlearning_rate\\x94G?\\xb9\\x99\\x99\\x99\\x99\\x99\\x9a\\x8c\\x04loss\\x94\\x8c\\x02ls\\x94h\\x80\\x8c\\x0cfriedman_mse\\x94h\\x85K\\x02h\\x86K\\x01h\\x87G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\tsubsample\\x94G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00h\\x88Nh\\x84K\\x03h\\x8aG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x8bNh\\x8fG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x04init\\x94Nh\\x18Nh\\x0eG?\\xec\\xcc\\xcc\\xcc\\xcc\\xcc\\xcd\\x8c\\x07verbose\\x94K\\x00h\\x89Nh\\x16\\x89h\\x8dh\\x8e\\x8c\\x13validation_fraction\\x94G?\\xb9\\x99\\x99\\x99\\x99\\x99\\x9a\\x8c\\x10n_iter_no_change\\x94Nh\\x15G?\\x1a6\\xe2\\xeb\\x1cC-h\\x1bh\\x1cubh\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA9D30>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x11t\\xc9y\\x96%\\x0b%\\xa8hD\\xd1vSA+\\x80\\x00sbh&h*hE\\x8c]<class \\'sklearn.ensemble._gb.GradientBoostingRegressor\\'>-802b4153-76d1-4468-a825-0b259679c974\\x94ubh\\x03)\\x81\\x94}\\x94(h\\x06h\\x07h\\x08\\x8c\\x18sklearn.ensemble._forest\\x94\\x8c\\x15RandomForestRegressor\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x0ebase_estimator\\x94h})\\x81\\x94}\\x94(h\\x80h\\x81h\\x82h\\x83h\\x84Nh\\x85K\\x02h\\x86K\\x01h\\x87G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x88Nh\\x18Nh\\x89Nh\\x8aG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x8bNh\\x8cNh\\x8dh\\x8eh\\x8fG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x1bh\\x1cubh\\xa8Kd\\x8c\\x10estimator_params\\x94(h\\x80h\\x84h\\x85h\\x86h\\x87h\\x88h\\x89h\\x8ah\\x8bh\\x18h\\x8ft\\x94\\x8c\\tbootstrap\\x94\\x88\\x8c\\toob_score\\x94\\x89hjNh\\x18Nh\\xafK\\x00h\\x16\\x89h\\x8cN\\x8c\\x0bmax_samples\\x94Nh\\x80h\\x81h\\x84Nh\\x85K\\x02h\\x86K\\x01h\\x87G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x88h\\x9ch\\x89Nh\\x8aG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x8bNh\\x8fG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h\\x1bh\\x1cubh\\x1d\\x8c)<sha256 HASH object @ 0x000001640BBA9B90>\\x94h\\x1fh\")\\x81\\x94}\\x94h%\\x8a\\x11\\xe8\\x05u\\tX\\xd7%\\xb7\\xf2D\\xd7\\xc6)@\\xb8\\xbe\\x00sbh&h*hE\\x8c]<class \\'sklearn.ensemble._forest.RandomForestRegressor\\'>-beb84029-c6d7-44f2-b725-d758097505e8\\x94ube.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-13 02:39:46,969] INFO - prefect.FlowRunner | Beginning Flow run for 'meta_model_flow'\n",
      "[2020-04-13 02:39:46,973] INFO - prefect.FlowRunner | Starting flow run.\n"
     ]
    }
   ],
   "source": [
    "from meta_model import MetaModel, meta_model_flow\n",
    "tinydb = TinyDB(\"db.json\")\n",
    "executor = DaskExecutor()\n",
    "meta_model = MetaModel(problem=\"regression\", db=tinydb,)\n",
    "meta_model_run = meta_model_flow.run(\n",
    "    train_data = \"transformed_train.df\",\n",
    "    valid_data = \"transformed_valid.df\",\n",
    "    train_target = \"survived\",\n",
    "    meta_model = meta_model,\n",
    "    executor = executor,\n",
    "    problem = \"binary classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a679600787ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'meta_model' is not defined"
     ]
    }
   ],
   "source": [
    "dir(meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#flow.visualize(flow_state=flow_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "db = TinyDB(\"db.json\")\n",
    "db.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Query()\n",
    "r = db.search(q.chunk == \"svdname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__and__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_generate_test',\n",
       " '_path',\n",
       " '_prepare_test',\n",
       " '_test',\n",
       " 'all',\n",
       " 'any',\n",
       " 'exists',\n",
       " 'hashval',\n",
       " 'matches',\n",
       " 'one_of',\n",
       " 'search',\n",
       " 'test']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(q.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Any"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow._sorted_tasks()[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__and__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__mifflin__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " 'auto_generated',\n",
       " 'bind',\n",
       " 'cache_for',\n",
       " 'cache_key',\n",
       " 'cache_validator',\n",
       " 'checkpoint',\n",
       " 'copy',\n",
       " 'inputs',\n",
       " 'is_equal',\n",
       " 'is_not_equal',\n",
       " 'log_stdout',\n",
       " 'logger',\n",
       " 'map',\n",
       " 'max_retries',\n",
       " 'name',\n",
       " 'not_',\n",
       " 'or_',\n",
       " 'outputs',\n",
       " 'result_handler',\n",
       " 'retry_delay',\n",
       " 'run',\n",
       " 'serialize',\n",
       " 'set_dependencies',\n",
       " 'set_downstream',\n",
       " 'set_upstream',\n",
       " 'skip_on_upstream_skip',\n",
       " 'slug',\n",
       " 'state_handlers',\n",
       " 'tags',\n",
       " 'timeout',\n",
       " 'trigger']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(flow._sorted_tasks()[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
