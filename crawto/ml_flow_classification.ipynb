{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-04-11T23:02:42.759Z",
     "iopub.status.busy": "2020-04-11T23:02:42.689Z",
     "iopub.status.idle": "2020-04-11T23:02:52.706Z",
     "shell.execute_reply": "2020-04-11T23:02:52.781Z"
    }
   },
   "outputs": [],
   "source": [
    "from prefect import Flow, Parameter, unmapped\n",
    "import pandas as pd\n",
    "from prefect.engine.executors import DaskExecutor\n",
    "from ml_flow import data_cleaning_flow\n",
    "from tinydb import TinyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "test= pd.read_csv(\"../data/titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-13 05:50:42,269] INFO - prefect.FlowRunner | Beginning Flow run for 'data_cleaning'\n",
      "[2020-04-13 05:50:42,274] INFO - prefect.FlowRunner | Starting flow run.\n",
      "[2020-04-13 05:50:42,538] INFO - prefect.TaskRunner | Task 'target': Starting task run...\n",
      "[2020-04-13 05:50:42,544] INFO - prefect.TaskRunner | Task 'input_data': Starting task run...\n",
      "[2020-04-13 05:50:42,583] INFO - prefect.TaskRunner | Task 'target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,609] INFO - prefect.TaskRunner | Task 'input_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,754] INFO - prefect.TaskRunner | Task 'problem': Starting task run...\n",
      "[2020-04-13 05:50:42,768] INFO - prefect.TaskRunner | Task 'problem': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,776] INFO - prefect.TaskRunner | Task 'features': Starting task run...\n",
      "[2020-04-13 05:50:42,785] INFO - prefect.TaskRunner | Task 'features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,807] INFO - prefect.TaskRunner | Task 'extract_problematic_features': Starting task run...\n",
      "[2020-04-13 05:50:42,815] INFO - prefect.TaskRunner | Task 'extract_problematic_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,830] INFO - prefect.TaskRunner | Task 'extract_nan_features': Starting task run...\n",
      "[2020-04-13 05:50:42,844] INFO - prefect.TaskRunner | Task 'extract_nan_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,862] INFO - prefect.TaskRunner | Task 'extract_undefined_features': Starting task run...\n",
      "[2020-04-13 05:50:42,869] INFO - prefect.TaskRunner | Task 'extract_undefined_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:42,900] INFO - prefect.TaskRunner | Task 'fit_transform_missing_indicator': Starting task run...\n",
      "[2020-04-13 05:50:42,913] INFO - prefect.TaskRunner | Task 'extract_categorical_features': Starting task run...\n",
      "['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']\n",
      "[3 8][2020-04-13 05:50:42,923] INFO - prefect.TaskRunner | Task 'extract_numeric_features': Starting task run...\n",
      "\n",
      "['missing_Age', 'missing_Embarked']\n",
      "[2020-04-13 05:50:42,994] INFO - prefect.TaskRunner | Task 'fit_transform_missing_indicator': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,001] INFO - prefect.TaskRunner | Task 'extract_categorical_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,009] INFO - prefect.TaskRunner | Task 'extract_numeric_features': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,029] INFO - prefect.TaskRunner | Task 'extract_train_valid_split': Starting task run...\n",
      "[2020-04-13 05:50:43,037] INFO - prefect.TaskRunner | Task 'extract_train_valid_split': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,062] INFO - prefect.TaskRunner | Task 'extract_train_data': Starting task run...\n",
      "[2020-04-13 05:50:43,069] INFO - prefect.TaskRunner | Task 'extract_valid_data': Starting task run...\n",
      "[2020-04-13 05:50:43,076] INFO - prefect.TaskRunner | Task 'extract_train_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,084] INFO - prefect.TaskRunner | Task 'extract_valid_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,130] INFO - prefect.TaskRunner | Task 'fit_target_transformer': Starting task run...\n",
      "[2020-04-13 05:50:43,131] INFO - prefect.TaskRunner | Task 'fit_numeric_imputer': Starting task run...\n",
      "[2020-04-13 05:50:43,133] INFO - prefect.TaskRunner | Task 'fit_categorical_imputer': Starting task run...\n",
      "[2020-04-13 05:50:43,140] INFO - prefect.TaskRunner | Task 'fit_target_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,150] INFO - prefect.TaskRunner | Task 'fit_numeric_imputer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,164] INFO - prefect.TaskRunner | Task 'fit_categorical_imputer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,198] INFO - prefect.TaskRunner | Task 'impute_numeric_df': Starting task run...\n",
      "[2020-04-13 05:50:43,209] INFO - prefect.TaskRunner | Task 'impute_numeric_df': Starting task run...\n",
      "[2020-04-13 05:50:43,217] INFO - prefect.TaskRunner | Task 'transform_target': Starting task run...\n",
      "[2020-04-13 05:50:43,254] INFO - prefect.TaskRunner | Task 'transform_target': Starting task run...\n",
      "[2020-04-13 05:50:43,257] INFO - prefect.TaskRunner | Task 'transform_categorical_data': Starting task run...\n",
      "[2020-04-13 05:50:43,267] INFO - prefect.TaskRunner | Task 'transform_categorical_data': Starting task run...\n",
      "[2020-04-13 05:50:43,269] INFO - prefect.TaskRunner | Task 'impute_numeric_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,277] INFO - prefect.TaskRunner | Task 'impute_numeric_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,285] INFO - prefect.TaskRunner | Task 'transform_target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,293] INFO - prefect.TaskRunner | Task 'transform_target': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,305] INFO - prefect.TaskRunner | Task 'transform_categorical_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,312] INFO - prefect.TaskRunner | Task 'transform_categorical_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,370] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 05:50:43,371] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 05:50:43,379] INFO - prefect.TaskRunner | Task 'fit_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 05:50:43,381] INFO - prefect.TaskRunner | Task 'fit_target_encoder': Starting task run...\n",
      "[2020-04-13 05:50:43,385] ERROR - prefect.TaskRunner | Unexpected error: AttributeError(\"'Series' object has no attribute 'to_feather'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\runner.py\", line 48, in inner\n",
      "    new_state = method(self, state, *args, **kwargs)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\task_runner.py\", line 883, in get_task_run_state\n",
      "    result = timeout_handler(\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\utilities\\executors.py\", line 185, in timeout_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"E:\\projects\\crawto\\crawto\\ml_flow.py\", line 281, in save_data\n",
      "    df.to_feather(path)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'Series' object has no attribute 'to_feather'\n",
      "[2020-04-13 05:50:43,387] ERROR - prefect.TaskRunner | Unexpected error: AttributeError(\"'Series' object has no attribute 'to_feather'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\runner.py\", line 48, in inner\n",
      "    new_state = method(self, state, *args, **kwargs)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\engine\\task_runner.py\", line 883, in get_task_run_state\n",
      "    result = timeout_handler(\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\utilities\\executors.py\", line 185, in timeout_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"E:\\projects\\crawto\\crawto\\ml_flow.py\", line 281, in save_data\n",
      "    df.to_feather(path)\n",
      "  File \"e:\\projects\\crawto\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'Series' object has no attribute 'to_feather'\n",
      "[2020-04-13 05:50:43,418] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Failed'\n",
      "[2020-04-13 05:50:43,432] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Failed'\n",
      "[2020-04-13 05:50:43,473] INFO - prefect.TaskRunner | Task 'fit_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-13 05:50:43,514] INFO - prefect.TaskRunner | Task 'fit_target_encoder': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,544] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 05:50:43,545] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': Starting task run...\n",
      "[2020-04-13 05:50:43,575] INFO - prefect.TaskRunner | Task 'target_encoder_transform': Starting task run...\n",
      "[2020-04-13 05:50:43,577] INFO - prefect.TaskRunner | Task 'target_encoder_transform': Starting task run...\n",
      "[2020-04-13 05:50:43,584] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,593] INFO - prefect.TaskRunner | Task 'transform_yeo_johnson_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,630] INFO - prefect.TaskRunner | Task 'target_encoder_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,632] INFO - prefect.TaskRunner | Task 'target_encoder_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,661] INFO - prefect.TaskRunner | Task 'merge_transformed_data': Starting task run...\n",
      "[2020-04-13 05:50:43,675] INFO - prefect.TaskRunner | Task 'merge_transformed_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,676] INFO - prefect.TaskRunner | Task 'merge_transformed_data': Starting task run...\n",
      "[2020-04-13 05:50:43,687] INFO - prefect.TaskRunner | Task 'merge_transformed_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:43,703] INFO - prefect.TaskRunner | Task 'fit_hbos_transformer': Starting task run...\n",
      "[2020-04-13 05:50:45,083] INFO - prefect.TaskRunner | Task 'fit_hbos_transformer': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,108] INFO - prefect.TaskRunner | Task 'hbos_transform': Starting task run...\n",
      "[2020-04-13 05:50:45,114] INFO - prefect.TaskRunner | Task 'hbos_transform': Starting task run...\n",
      "[2020-04-13 05:50:45,122] INFO - prefect.TaskRunner | Task 'hbos_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,130] INFO - prefect.TaskRunner | Task 'hbos_transform': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,164] INFO - prefect.TaskRunner | Task 'merge_hbos_df': Starting task run...\n",
      "[2020-04-13 05:50:45,165] INFO - prefect.TaskRunner | Task 'merge_hbos_df': Starting task run...\n",
      "[2020-04-13 05:50:45,173] INFO - prefect.TaskRunner | Task 'merge_hbos_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,181] INFO - prefect.TaskRunner | Task 'merge_hbos_df': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,198] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 05:50:45,215] INFO - prefect.TaskRunner | Task 'save_data': Starting task run...\n",
      "[2020-04-13 05:50:45,226] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:45,234] INFO - prefect.TaskRunner | Task 'save_data': finished task run for task with final state: 'Success'\n",
      "[2020-04-13 05:50:47,233] INFO - prefect.FlowRunner | Flow run FAILED: some reference tasks failed.\n"
     ]
    }
   ],
   "source": [
    "executor = DaskExecutor()\n",
    "data_cleaner = data_cleaning_flow.run(\n",
    "    input_data= input_df, \n",
    "    problem=\"binary classification\", \n",
    "    target = \"Survived\", \n",
    "    features = \"infer\",\n",
    "    executor=executor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Flow.run did not receive the following required parameters: problem",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-16b1e942ed42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmeta_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMetaModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_model_flow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDaskExecutor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m meta_model_run = meta_model_flow.run(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"transformed_train.df\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"transformed_valid.df\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\projects\\crawto\\.venv\\lib\\site-packages\\prefect\\core\\flow.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameters, run_on_schedule, runner_cls, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m             \u001b[0mfmt_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1011\u001b[0m                 \"Flow.run did not receive the following required parameters: {}\".format(\n\u001b[0;32m   1012\u001b[0m                     \u001b[0mfmt_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Flow.run did not receive the following required parameters: problem"
     ]
    }
   ],
   "source": [
    "from meta_model import MetaModel, meta_model_flow\n",
    "executor = DaskExecutor()\n",
    "meta_model_run = meta_model_flow.run(\n",
    "    train_data = \"transformed_train.df\",\n",
    "    valid_data = \"transformed_valid.df\",\n",
    "    train_target = \"train_target.df\",\n",
    "    tinydb = \"db.json\",\n",
    "    #problem = \"binary classification\",\n",
    "    executor = executor,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_model import MetaModel, meta_model_flow\n",
    "from tinydb import TinyDB\n",
    "tinydb = TinyDB(\"db.json\")\n",
    "meta_model = MetaModel(problem=\"regression\", db=tinydb,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(\"data_visualization\") as flow:\n",
    "    transformed_train_df = pd.read_feather(\"transformed_train.df\")\n",
    "    transformed_valid_df = pd.read_feather(\"transformed_valid.df\")\n",
    "    svd = fit_svd(transformed_train_df)\n",
    "    svd_train = svd_transform(svd, transformed_train_df, \"transformed_train_df\",tinydb)\n",
    "    svd_valid = svd_transform(svd, transformed_valid_df, \"transformed_valid_df\",tinydb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#flow.visualize(flow_state=flow_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "db = TinyDB(\"db.json\")\n",
    "db.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Query()\n",
    "r = db.search(q.chunk == \"svdname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(q.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow._sorted_tasks()[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(flow._sorted_tasks()[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
